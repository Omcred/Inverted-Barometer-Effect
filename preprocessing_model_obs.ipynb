{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e0a5fb8-eef4-4bc8-81c6-5f23c192ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import utide\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62608478-bd91-47c6-b54d-044b8b16461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_start=1958\n",
    "year_end=2014\n",
    "\n",
    "model_loc = ['SEATTLE', 'SANDY_HOOK', 'WELLINGTON_HARBOUR']\n",
    "\n",
    "gesla_loc = {'SEATTLE': \"seattle-9447130-usa-noaa\",\n",
    "\t\t  'SANDY_HOOK': \"sandy_hook-8531680-usa-noaa\",\n",
    "          'WELLINGTON_HARBOUR': \"wellington-071a-nzl-uhslc\"}\n",
    "\n",
    "ds = xr.open_dataset(\"/vftmp/Olivia.Mcredmond/data/model_timeseries_odiv181.nc\",decode_times=True)\n",
    "da = xr.open_dataset(\"/vftmp/Olivia.Mcredmond/data/model_timeseries_odiv2.nc\",decode_times=True)\n",
    "odiv181_pressure= xr.open_dataset(\"/vftmp/Olivia.Mcredmond/data/model_timeseries_psl_odiv181.nc\")\n",
    "odiv2_pressure= xr.open_dataset(\"/vftmp/Olivia.Mcredmond/data/model_timeseries_psl_odiv2.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df470b79-0932-4867-bea4-803c1db43840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model_data(location_name, year_start,year_end):\n",
    "    model_ts = ds[location_name]\n",
    "    model_ta = da[location_name]\n",
    "    \n",
    "    model2= model_ta.sel(time=slice(f\"{year_start}-01-01\", f\"{year_end}-12-31\"))\n",
    "    model2= model2-model2.mean()\n",
    "    model2['time']=[pd.to_datetime(str(np.array(model2.time[i])), format='mixed', yearfirst=True) for i in range(0,len(np.array(model2)))]\n",
    "    \n",
    "    model181= model_ts.sel(time=slice(f\"{year_start}-01-01\", f\"{year_end}-12-31\"))\n",
    "    model181=model181-model181.mean()\n",
    "    model181['time']=[pd.to_datetime(str(np.array(model181.time[i])), format='mixed', yearfirst=True) for i in range(0,len(np.array(model181)))]\n",
    "    \n",
    "    model2_IB = -1*(odiv2_pressure[location_name].sel(time=slice(f\"{year_start}-01-01\", f\"{year_end}-12-31\")))*0.01*0.1/(1.02*9.8)\n",
    "    model2_IB['time']=[pd.to_datetime(str(np.array(model2_IB.time[i])), format='mixed', yearfirst=True) for i in range(0,len(np.array(model2_IB)))]\n",
    "    model2_IB = model2+(model2_IB-model2_IB.mean())\n",
    "    \n",
    "    model181_IB=-1*(odiv181_pressure[location_name].sel(time=slice(f\"{year_start}-01-01\", f\"{year_end}-12-31\")))*0.01*0.1/(1.02*9.8)\n",
    "    model181_IB['time']=[pd.to_datetime(str(np.array(model181_IB.time[i])), format='mixed', yearfirst=True) for i in range(0,len(np.array(model181_IB)))]\n",
    "    model181_IB = model181+np.array(model181_IB-model181_IB.mean())\n",
    "    \n",
    "    model_output=xr.Dataset(\n",
    "        data_vars={\"model_181\":model181,\n",
    "                   \"model_2\":model2,\n",
    "                   \"model_2_IB\":model2_IB,\n",
    "                   \"model_181_IB\":model181_IB},\n",
    "        coords={\"time\":model2.time},\n",
    "        attrs={'location_name':str({location_name})})\n",
    "    model_output.to_netcdf(f'/vftmp/Olivia.Mcredmond/data/cdf files/{location_name}_MODEL_PROCESSED.nc')\n",
    "\t\t## this will save ST_PETERSBURG_MODEL_PROCESSED.nc as a\n",
    "## file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5322d7b3-f9dc-4814-aab6-d61305e3ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gesla_data(location_name, year_start, year_end):\n",
    "    gesla_name = gesla_loc[location_name]\n",
    "    data = f'../data/{gesla_name}'\n",
    "    with open(data) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for row in lines:\n",
    "        word = 'LATITUDE'\n",
    "        if row.find(word) != -1:\n",
    "            lat= float((row.split(\"DE \")[1]).strip())\n",
    "            break\n",
    "    names = [\"date\", \"hour\", \"sealevel\", \"flag\", \"use_flag\"]\n",
    "    #Read data from file\n",
    "    obs = pd.read_csv(\n",
    "        data,\n",
    "        names=names,\n",
    "        skiprows=41, # to skip the large header and documentation, would need to check whether this is the same for every time series\n",
    "        skipinitialspace=True,\n",
    "        delim_whitespace=True,\n",
    "        na_values=\"-99.9999\",\n",
    "    )\n",
    "    \n",
    "    #Turn bad data to nan using flags\n",
    "    good_data = obs['use_flag'] == 1\n",
    "    bad_data = obs['use_flag'] == 0\n",
    "    obs.loc[bad_data, \"sealevel\"] = np.nan\n",
    "    \n",
    "    #Subtract mean and define datetime\n",
    "    obs[\"anomaly\"] = obs['sealevel'] - obs['sealevel'].mean()\n",
    "    obs['datetime'] = pd.to_datetime(obs['date'] + ' ' +obs['hour'])\n",
    "\n",
    "    t1 = (obs['datetime'] >= f'{year_start}-1-1') & (obs['datetime'] <= f'{year_end}-12-31')\n",
    "    obs_t1=obs.loc[t1]\n",
    "    print(\"pt 1\")\n",
    "    coef = utide.solve(\n",
    "        obs_t1.datetime,\n",
    "        obs_t1.anomaly,\n",
    "        lat= lat,\n",
    "        method=\"ols\",\n",
    "        conf_int=\"MC\",\n",
    "        verbose=False,\n",
    "       constit=['M2', 'K1', 'O1', 'S2', 'P1', 'N2', 'K2'] \n",
    "    )\n",
    "    # Reconstruct using coefficients defined from utide for a defined time length ('obs_1.datetime' in this case)\n",
    "    tide = utide.reconstruct(obs_t1.datetime, coef, verbose=False)\n",
    "    obs_t1['sealevel_tr'] = obs_t1.anomaly - tide.h\n",
    "    obs_t1['datetime'] = pd.to_datetime(obs_t1['datetime'])\n",
    "    # Set the datetime column as the index\n",
    "    obs_t1.set_index('datetime', inplace=True)\n",
    "    daily_avg=xr.Dataset(\n",
    "        data_vars={\"sealevel_tr\":obs_t1['sealevel_tr'].resample('D').mean()},\n",
    "        coords={\"datetime\":obs_t1.index},\n",
    "        attrs={'location_name':str({location_name})})\n",
    "    daily_avg.to_netcdf(f'/vftmp/Olivia.Mcredmond/data/cdf files/{location_name}_GESLA_PROCESSED.nc')\n",
    "## You can then compare ST_PETERSBURG_MODEL_PROCESSED.nc and ST_PETERSBURG_GESLA_PROCESSED.nc!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c72b329-d0cd-47d5-9ef4-8fdf56da93ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model_loc:\n",
    "\tprocess_model_data(i,year_start,year_end)\n",
    "\t#process_gesla_data(i,year_start,year_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcbd64-3652-4b19-a7fa-840c37e29b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
